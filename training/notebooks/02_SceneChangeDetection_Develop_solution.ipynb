{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. **Develop solution:**  Scene Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1. Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "In this part of the series, we will train an Machine learning or Deep learning based model (implemented in Keras) in for Anomaly Detection in Survaliance cameras. The trained model will be evaluated on pre-labeled and anonymized dataset.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2.  Install the modeling requirements and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we'll need to **install some libraries** that are not part of our container image. Normally, **Red Hat OpenShift Data Science** or **Red Hat Open Data Hub** is already taking care of this for you, based on what it detects in the code. **Red Hat OpenShift Data Science** or **Red Hat Open Data Hub** will reinstall all those libraries for you every time you launch the notebook!\n",
    "\n",
    "In case you're using this notebook in a different environment, or just to make sure everything is ready, you can run the following cell to install  Keras (an abstraction layer over Tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQmKthrSBCld",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements1.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3.  Importing the needed libraries and packages\n",
    "Of course, we'll need to import various packages. They are either built in the notebook image you are running, or have been installed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys; sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/opt/app-root/src/predictive-maint/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import requests\n",
    "from training.src.dataloading.read_dataset import readData\n",
    "from training.src.features.data_preprocessing import preprocessData\n",
    "from training.src.visualization.visualize import visualizeData\n",
    "from training.src.modules.build_model import buildModel\n",
    "from training.src.modules.train_model import  trainModel,MLflow\n",
    "from training.src.modules.predict_model import predictor\n",
    "from training.src.hyper_parameters.hps import get_hyper_paras\n",
    "# from training.src.github_commands.git_utils import gitCommands\n",
    "from training.src.deploy_app.deploy import deployApplication\n",
    "\n",
    "\n",
    "\n",
    "import os, sys; sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import mlflow\n",
    "import warnings\n",
    "import numpy as np\n",
    "from minio import Minio\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4. Initialize some hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SPLITE_RATE, OUTPUT_FEATURE_NAME, BATCH_SIZE, IMAGE_SIZE, INPUT_SHAPE, DROP_OUT_RATE, EPOCHS, SEED, TRAIN_DATA_FLAG, FINE_TUNE_FLAG = get_hyper_paras()\n",
    "SPLITE_RATE, OUTPUT_FEATURE_NAME, BATCH_SIZE, IMAGE_SIZE, INPUT_SHAPE, DROP_OUT_RATE, EPOCHS, SEED, TRAIN_DATA_FLAG, FINE_TUNE_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4.1. Define a client to read from Minio S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_s3_server():\n",
    "    minioClient = Minio(os.environ['MLFLOW_S3_ENDPOINT'],\n",
    "                    access_key=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                    secret_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "                    secure=False)\n",
    "\n",
    "    return minioClient\n",
    "client = get_s3_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4.2. SetUp MLFlow to track the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlflow = MLflow(mlflow).SetUp_Mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.5. Reading the  Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds, num_classes = readData(dataPath = os.environ['DATA_PATH'],imageSize = IMAGE_SIZE, batchSize = BATCH_SIZE, seed = SEED  ).generateData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## 1.6.  Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to resize the data to make them ready for feeing to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds, data_augmentation  = preprocessData(trainDs = train_ds,testDs = test_ds,valDs = val_ds,numClasses = num_classes,augFlag=True,height=IMAGE_SIZE[0],width=IMAGE_SIZE[1],batchSize = BATCH_SIZE).dataPreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.7.  Design and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = buildModel(dataAugmentation = data_augmentation, inputShape=INPUT_SHAPE, numClasses=num_classes, topDropoutRate=DROP_OUT_RATE).setupModel()\n",
    "# clf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.8.  Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = trainModel(model = clf, trainDs = train_ds, valDs = val_ds, batchSize=32, epochs=3, mlflow=mlflow).ModelTraining()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. **Validate solution based on validation data**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictor(model = clf,data=test, modelType=model_Type).predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.system('rm -rf ' +os.environ['DATA_PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. **Deploy solution as an app with seldon** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deployApplication().deployApp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. **Test the Credit Fraud Detection app** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.1. Get the app route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_route = 'http://model-1-pred-demo-fmv3.apps.dbs-indo-1.apac-1.rht-labs.com/api/v1.0/predictions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.2. Read the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "im = Image.open(\"sample.jpg\")\n",
    "img = np.asarray(im)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.3. Make a decision based on input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "import requests\n",
    "from json import dumps\n",
    "my_image = 'sample.jpg'\n",
    "with open(my_image, \"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "content = {\"data\":\n",
    "          {\n",
    "                \"names\":\n",
    "                    [\"Sentence\"],\n",
    "             \"ndarray\": img.tolist()\n",
    "\n",
    "          }\n",
    "        }\n",
    "json_data = dumps(content)\n",
    "headers = {\"Content-Type\" : \"application/json\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "r = requests.post(my_route , data=json_data, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "\n",
    "class Predictor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loaded = False\n",
    "        self.labels = [\"Blocked\",\"Blured\",\"Changed_View\",\"Normal\", \"Others\"]\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        print(\"Loading model\",os.getpid())\n",
    "        self.model = tf.keras.models.load_model( '../deploy/model.h5', compile=False)\n",
    "        print(\"Model Loaded!\")\n",
    "        self.loaded = True\n",
    "        print(\"Loaded model\")\n",
    "\n",
    "    def predict_raw(self, data):\n",
    "        print('step 00')\n",
    "        if data:\n",
    "            float_array = tf.constant(np.array(data))\n",
    "            float_array = tf.expand_dims(float_array, 0)\n",
    "            print('step 01')\n",
    "\n",
    "        print ('step1......')\n",
    "        print(float_array.shape)\n",
    "\n",
    "      \n",
    "        if not self.loaded:\n",
    "            self.load()\n",
    "        try:\n",
    "            result = self.model.predict(float_array) \n",
    "        except Exception as e:\n",
    "            print(traceback.format_exception(*sys.exc_info()))\n",
    "        \n",
    "        ######\n",
    "\n",
    "\n",
    "        \n",
    "        json_results = {}\n",
    "        arg_max_result = tf.math.argmax(result,axis=1)\n",
    "        print(\"1\"*50)\n",
    "        print(arg_max_result)\n",
    "        json_results[\"Predicted Class: \"] = str(self.labels[int(arg_max_result)])\n",
    "        print(\"2\"*50)\n",
    "        \n",
    "        json_results[\"Predicted Label: \"] = json.dumps(arg_max_result.numpy(), cls=JsonSerializer)\n",
    "        print(\"3\"*50)\n",
    "        json_results[\"Predicted Class Prob: \"] = json.dumps(str(np.max(result, axis=1)), cls=JsonSerializer)\n",
    "        print(\"4\"*50)\n",
    "        json_results[\"All Probs: \"] = json.dumps(result, cls=JsonSerializer)\n",
    "        \n",
    "        \n",
    "        print(json_results)\n",
    "        print (result)\n",
    "        print(json.dumps(json_results))\n",
    "        return json.dumps(json_results)\n",
    "        # return json.dumps(result.numpy(), cls=JsonSerializer)\n",
    "\n",
    "class JsonSerializer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (\n",
    "        np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Predictor().predict_raw(img.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Thank you for your time!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from json import dumps\n",
    "my_image = 'sample.jpg'\n",
    "with open(my_image, \"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "content = {\"image\": encoded_image}\n",
    "json_data = dumps(content)\n",
    "\n",
    "headers = {\"Content-Type\" : \"application/json\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "r = requests.post(my_route + '/predictions', data=json_data, headers=headers)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segmentation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
